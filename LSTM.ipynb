{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.9 (default, Oct 18 2020, 22:55:02) \n",
      "[Clang 10.0.1 (clang-1001.0.46.4)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = \"data/nesmdb_token\"\n",
    "TRAIN_DIR = f\"{DATA_ROOT}/train\"\n",
    "VALID_DIR = f\"{DATA_ROOT}/valid\"\n",
    "TEST_DIR = f\"{DATA_ROOT}/test\"\n",
    "\n",
    "VOCAB_PATH = \"data/vocab.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 632\n"
     ]
    }
   ],
   "source": [
    "train_files = glob.glob(f\"{TRAIN_DIR}/*.txt\")\n",
    "valid_files = glob.glob(f\"{VALID_DIR}/*.txt\")\n",
    "test_files = glob.glob(f\"{TEST_DIR}/*.txt\")\n",
    "\n",
    "vocab = utils.load_data(VOCAB_PATH)\n",
    "token_to_index = {token: i for i, token in enumerate(vocab)}\n",
    "\n",
    "VOCAB_SIZE = len(vocab)\n",
    "print(f\"Vocab size: {VOCAB_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(tokens):\n",
    "    \"\"\"Convert a list of tokens to a tensor of indices.\"\"\"\n",
    "    return torch.LongTensor([token_to_index[token] for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicGenerator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, rnn_type=\"gru\", num_layers=1):\n",
    "        assert rnn_type in [\"gru\", \"lstm\"]\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.rnn_type = rnn_type\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embeddings = nn.Embedding(input_size, hidden_size)\n",
    "        self.rnn = (\n",
    "            nn.LSTM(hidden_size, hidden_size, num_layers) if rnn_type == \"lstm\"\n",
    "            else nn.GRU(hidden_size, hidden_size, num_layers)\n",
    "        )\n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Note: Instead of using a one-hot encoding for each token and then\n",
    "        # multiplying it by a weight matrix, we directly store an embedding\n",
    "        # for every token in x. This is equivalent but speeds up computation.\n",
    "        x = self.embeddings(x)\n",
    "\n",
    "        # Add batch dimension.\n",
    "        x = x.unsqueeze(0)\n",
    "\n",
    "        # Forward pass through RNN and FC layer.\n",
    "        x, _ = self.rnn(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        # Remove batch dimension.\n",
    "        x = x.squeeze(0)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 64\n",
    "NUM_EPOCHS = 300\n",
    "LEARNING_RATE = 0.1\n",
    "LOG_EVERY_N = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MusicGenerator(\n",
    "    input_size=VOCAB_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    output_size=VOCAB_SIZE,\n",
    "    rnn_type=\"lstm\",\n",
    ")\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [\n",
    "    to_tensor(utils.load_data(file))\n",
    "    for file in train_files[:10]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "febb9c2c50454777bad4ab49cf2accbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=300.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 \t Avg loss: 1.8947256247202555\n",
      "Epoch: 100 \t Avg loss: 1.6567293326059978\n",
      "Epoch: 150 \t Avg loss: 1.5174024820327758\n",
      "Epoch: 200 \t Avg loss: 1.4069296638170878\n",
      "Epoch: 250 \t Avg loss: 1.3186911582946776\n",
      "Epoch: 300 \t Avg loss: 1.2488004485766093\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tr = trange(1, NUM_EPOCHS + 1)\n",
    "for epoch in tr:\n",
    "    total_loss = 0.0\n",
    "    for seq in train_data:\n",
    "        # Reset gradients.\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Prepare inputs and targets.\n",
    "        x = seq[:-1]\n",
    "        y = seq[1:]\n",
    "        \n",
    "        # Forward prop.\n",
    "        y_hat = model(x)\n",
    "        \n",
    "        # Compute loss.\n",
    "        loss = loss_function(y_hat, y)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Back prop.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters.\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = total_loss / len(seq)\n",
    "    if epoch % LOG_EVERY_N == 0:\n",
    "        tr.write(f\"Epoch: {epoch} \\t Avg loss: {avg_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataset):\n",
    "    with torch.no_grad(): \n",
    "        total_loss = 0.0\n",
    "        for seq in tqdm(dataset):\n",
    "            x = seq[:-1]\n",
    "            y = seq[1:]\n",
    "            y_hat = model(x)\n",
    "            loss = loss_function(y_hat, y)\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(seq)\n",
    "        return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06680433337505047"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(model, [t0, t1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
