{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from music21 import converter\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from IPython.display import display, Audio\n",
    "\n",
    "from utils import decode_midi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicRNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, rnn_type=\"lstm\", num_layers=1, dropout=0.0):\n",
    "        assert rnn_type in [\"lstm\", \"gru\"]\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.rnn_type = rnn_type\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        if rnn_type == \"lstm\":\n",
    "            self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers, dropout=dropout, batch_first=True) \n",
    "        else:\n",
    "            self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "        self.hidden_state = None\n",
    "\n",
    "    def forward(self, x, reset_hidden_state=True):\n",
    "        x = self.embeddings(x)\n",
    "        if reset_hidden_state:\n",
    "            x, _ = self.rnn(x)\n",
    "        else:\n",
    "            x, self.hidden_state = self.rnn(x, self.hidden_state)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def generate_sequence(self, beam_width=1, seq_start=None, max_length=1024, **kwargs):\n",
    "        if not seq_start:\n",
    "            seq_start = [BOS_TOKEN]\n",
    "        seq = seq_start.copy()\n",
    "        with torch.no_grad():\n",
    "            # Generate k most likely tokens\n",
    "            prev_top_seq, next_top_seq = [], []\n",
    "            prev_top_seq = [(seq, 0.0)]\n",
    "            \n",
    "            kpos_next_tokens = self._generate_next_token(\n",
    "                candidates=torch.LongTensor(prev_top_seq).to(device),\n",
    "                reset_hidden=True,\n",
    "                **kwargs\n",
    "            )\n",
    "            \n",
    "            next_top_seq.extend(kpos_next_tokens)\n",
    "            \n",
    "            while len(seq) <= max_length:\n",
    "                for sequence in prev_top_seq:\n",
    "                    kpos_next_tokens = self._generate_next_token(\n",
    "                        candidates=torch.LongTensor([kpos_next_tokens]).to(device),\n",
    "                        reset_hidden=False,\n",
    "                        **kwargs\n",
    "                    )\n",
    "                    \n",
    "                    next_top_seq.extend(kpos_next_tokens)\n",
    "                \n",
    "                next_top_seq.sort(reverse=True, key=lambda tup: tup[1])\n",
    "                prev_top_seq = next_top_seq[:self.beam_width]\n",
    "                next_top_seq = []\n",
    "                \n",
    "                seq.append(next_token)\n",
    "        \n",
    "        n = min(len(prev_top_seq), max_length)\n",
    "        return prev_top_seq[:n]\n",
    "    \n",
    "    def _generate_next_token(self, candidates, reset_hidden=False, temp=1.0, topk=5, argmax=False):\n",
    "        # The model expects a batch input, so we add a fake batch dimension.\n",
    "        # Removed the fake batch dimension (.unsqueeze(0)) because we have batch of beam_width tokens\n",
    "        import pdb;\n",
    "        pdb.set_trace()\n",
    "        model_input = np.array([tup[0] for tup in candidates]) # Previous sequences: [(seq , score), (seq2, score), (seq3, score)]\n",
    "        # Then, we need to remove the fake batch dimension from the output.\n",
    "        # Also removed the (.squeeze(0)) for similar reasons\n",
    "        model_output = self(model_input, reset_hidden)\n",
    "        \n",
    "        # Apply softmax to top beam_width tokens\n",
    "        \n",
    "        for i in range(len(candidates)):\n",
    "            next_token_probs = [(candidates[i].first + token, candidates[i].second + np.log(score)) for score in F.softmax(model_output[:, i] / temp, dim=0)]\n",
    "        \n",
    "        if argmax:\n",
    "            # Keep top beam_width tokens\n",
    "            kpos_next_tokens = torch.topk(next_token_probs, self.beam_width)\n",
    "        else:\n",
    "            # TO-DO: Implement beam search for this case too\n",
    "            top_tokens = torch.topk(next_token_probs, topk)\n",
    "            top_indices = top_tokens.indices\n",
    "            top_probs = top_tokens.values\n",
    "            top_probs /= torch.sum(top_probs)\n",
    "            kpos_next_tokens = np.random.choice(top_indices.cpu().numpy(), p=top_probs.cpu().numpy())\n",
    "        return kpos_next_tokens\n",
    "    \n",
    "    def prune_notes():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = \"data/pop_pickle\"\n",
    "N_SAMPLES = 909\n",
    "VOCAB_SIZE = 390\n",
    "BOS_TOKEN = VOCAB_SIZE - 2\n",
    "PAD_TOKEN = VOCAB_SIZE - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for i in range(N_SAMPLES):\n",
    "    file_name = str(i + 1).zfill(3) + \".pickle\"\n",
    "    path = os.path.join(DATA_ROOT, file_name)\n",
    "    with open(path, \"rb\") as f:\n",
    "        seq = pickle.load(f)\n",
    "        seq_tensor = torch.LongTensor(seq)\n",
    "    dataset.append(seq_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 819 \t Val: 45 \t Test: 45\n"
     ]
    }
   ],
   "source": [
    "N_VAL = N_TEST = int(0.05 * 909)\n",
    "N_TRAIN = 909 - (N_VAL + N_TEST)\n",
    "\n",
    "train_songs = dataset[:N_TRAIN]\n",
    "val_songs = dataset[N_TRAIN:N_TRAIN+N_VAL]\n",
    "test_songs = dataset[N_TRAIN+N_VAL:]\n",
    "\n",
    "print(f\"Train: {len(train_songs)} \\t Val: {len(val_songs)} \\t Test: {len(test_songs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIRST_N = 125\n",
    "IDX = 10\n",
    "\n",
    "primer = test_songs[IDX][:FIRST_N].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm = MusicRNN(\n",
    "    embedding_dim=64,\n",
    "    hidden_dim=256,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    rnn_type=\"lstm\",\n",
    "    num_layers=2,\n",
    "    dropout=0.5,\n",
    ").to(device)\n",
    "\n",
    "sd = torch.load(\"models/lstm(64,256),lr=0.001,bsz=256,nepochs=250,sl=1024,dropout=0.5,t=2020-11-13_14:30.pt\")[\"model_state_dict\"]\n",
    "lstm.load_state_dict(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "not a sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-4577b91f5022>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprimer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_songs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIDX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFIRST_N\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcontinuation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdisplay_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontinuation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-2cc50a6a0262>\u001b[0m in \u001b[0;36mgenerate_sequence\u001b[0;34m(self, beam_width, seq_start, max_length, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             kpos_next_tokens = self._generate_next_token(\n\u001b[0;32m---> 42\u001b[0;31m                 \u001b[0mcandidates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_top_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m                 \u001b[0mreset_hidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: not a sequence"
     ]
    }
   ],
   "source": [
    "primer = test_songs[IDX][:FIRST_N].tolist()\n",
    "continuation = lstm.generate_sequence(primer, temp=1.0, topk=128)\n",
    "display_audio(continuation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
