{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.0 (default, Nov 13 2020, 00:03:17) \n",
      "[Clang 12.0.0 (clang-1200.0.32.27)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-f46451c348f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmusic21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from music21 import converter\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from IPython.display import display, Audio\n",
    "\n",
    "from utils import decode_midi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = \"data/pop_pickle\"\n",
    "N_SAMPLES = 909\n",
    "VOCAB_SIZE = 390\n",
    "BOS_TOKEN = VOCAB_SIZE - 2\n",
    "PAD_TOKEN = VOCAB_SIZE - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for i in range(N_SAMPLES):\n",
    "    file_name = str(i + 1).zfill(3) + \".pickle\"\n",
    "    path = os.path.join(DATA_ROOT, file_name)\n",
    "    with open(path, \"rb\") as f:\n",
    "        seq = pickle.load(f)\n",
    "        seq_tensor = torch.LongTensor(seq)\n",
    "    dataset.append(seq_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_lens = [len(seq) for seq in dataset]\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(seq_lens, bins=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_VAL = N_TEST = int(0.05 * 909)\n",
    "N_TRAIN = 909 - (N_VAL + N_TEST)\n",
    "\n",
    "train_songs = dataset[:N_TRAIN]\n",
    "val_songs = dataset[N_TRAIN:N_TRAIN+N_VAL]\n",
    "test_songs = dataset[N_TRAIN+N_VAL:]\n",
    "\n",
    "print(f\"Train: {len(train_songs)} \\t Val: {len(val_songs)} \\t Test: {len(test_songs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(seq, new_seq_len):\n",
    "    \"\"\"\n",
    "    Convert a sequence into a batch of equal-length sequences,\n",
    "    and prepend a BOS token at the start of every sequence.\n",
    "    \n",
    "    Unless new_seq_len perfectly divides the number of tokens\n",
    "    in seq, the last sequence will be padded with PAD_TOKENs.\n",
    "    \"\"\"\n",
    "    n = seq.numel() % new_seq_len\n",
    "    pad_size = 0 if n == 0 else new_seq_len - n\n",
    "    pad = torch.full(size=(pad_size,), fill_value=PAD_TOKEN)\n",
    "    seq = torch.cat([seq, pad])\n",
    "    assert seq.numel() % new_seq_len == 0\n",
    "    num_batches = int(seq.numel() / new_seq_len)\n",
    "    batch = seq.view(num_batches, -1)\n",
    "    bos = torch.full(size=(num_batches, 1), fill_value=BOS_TOKEN)\n",
    "    batch = torch.cat([bos, batch], dim=1)    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 1024\n",
    "\n",
    "train_data = torch.cat([batchify(seq, SEQ_LEN) for seq in train_songs])\n",
    "val_data = torch.cat([batchify(seq, SEQ_LEN) for seq in val_songs])\n",
    "test_data = torch.cat([batchify(seq, SEQ_LEN) for seq in test_songs])\n",
    "\n",
    "print(f\"Train: {len(train_data)} \\t Val: {len(val_data)} \\t Test: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the initial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicRNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, rnn_type=\"lstm\", num_layers=1, dropout=0.0):\n",
    "        assert rnn_type in [\"lstm\", \"gru\"]\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.rnn_type = rnn_type\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.beam_width = 10\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        if rnn_type == \"lstm\":\n",
    "            self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers, dropout=dropout, batch_first=True) \n",
    "        else:\n",
    "            self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "        self.hidden_state = None\n",
    "\n",
    "    def forward(self, x, reset_hidden_state=True):\n",
    "        x = self.embeddings(x)\n",
    "        if reset_hidden_state:\n",
    "            x, _ = self.rnn(x)\n",
    "        else:\n",
    "            x, self.hidden_state = self.rnn(x, self.hidden_state)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def generate_sequence(self, seq_start=None, max_length=1024, beam_width, **kwargs):\n",
    "        if not seq_start:\n",
    "            seq_start = [BOS_TOKEN]\n",
    "        seq = seq_start.copy()\n",
    "        with torch.no_grad():\n",
    "            # Generate k most likely tokens\n",
    "            prev_top_seq, next_top_seq = [], []\n",
    "            prev_top_seq = [(seq, 0.0)]\n",
    "            \n",
    "            kpos_next_tokens = self._generate_next_token(\n",
    "                next_input=torch.LongTensor(seq).to(device),\n",
    "                reset_hidden=True,\n",
    "                **kwargs\n",
    "            )\n",
    "            \n",
    "            next_top_seq.extend(kpos_next_tokens)\n",
    "            \n",
    "            while len(seq) <= max_length:\n",
    "                for sequence in prev_top_seq:\n",
    "                    kpos_next_tokens = self._generate_next_token(\n",
    "                        next_input=torch.LongTensor([kpos_next_tokens]).to(device),\n",
    "                        reset_hidden=False,\n",
    "                        **kwargs\n",
    "                    )\n",
    "                    \n",
    "                    next_top_seq.extend(kpos_next_tokens)\n",
    "                \n",
    "                next_top_seq.sort(reverse=True, key=lambda tup: tup[1])\n",
    "                prev_top_seq = next_top_seq[:self.beam_width]\n",
    "                next_top_seq = []\n",
    "                \n",
    "                seq.append(next_token)\n",
    "        \n",
    "        n = min(len(prev_top_seq), max_length)\n",
    "        return prev_top_seq[:n]\n",
    "    \n",
    "    def _generate_next_token(self, candidates, reset_hidden=False, temp=1.0, topk=5, argmax=False):\n",
    "        # The model expects a batch input, so we add a fake batch dimension.\n",
    "        # Removed the fake batch dimension (.unsqueeze(0)) because we have batch of beam_width tokens\n",
    "        model_input = np.array([tup[0] for tup in candidates]) # Previous sequences: [(seq , score), (seq2, score), (seq3, score)]\n",
    "        # Then, we need to remove the fake batch dimension from the output.\n",
    "        # Also removed the (.squeeze(0)) for similar reasons\n",
    "        model_output = self(model_input, reset_hidden)\n",
    "        \n",
    "        # Apply softmax to top beam_width tokens\n",
    "        \n",
    "        for i in range(len(candidates)):\n",
    "            next_token_probs = [(candidates[i].first + token, candidates[i].second + np.log(score)) for score in F.softmax(model_output[:, i] / temp, dim=0)]\n",
    "        \n",
    "        if argmax:\n",
    "            # Keep top beam_width tokens\n",
    "            kpos_next_tokens = torch.topk(next_token_probs, self.beam_width)\n",
    "        else:\n",
    "            # TO-DO: Implement beam search for this case too\n",
    "            top_tokens = torch.topk(next_token_probs, topk)\n",
    "            top_indices = top_tokens.indices\n",
    "            top_probs = top_tokens.values\n",
    "            top_probs /= torch.sum(top_probs)\n",
    "            kpos_next_tokens = np.random.choice(top_indices.cpu().numpy(), p=top_probs.cpu().numpy())\n",
    "        return kpos_next_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, data_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0.0\n",
    "        for batch in data_loader:\n",
    "            batch = batch.to(device)\n",
    "            x = batch[:, :-1]\n",
    "            y = batch[:, 1:]\n",
    "            y_hat = model(x)\n",
    "            y_hat = y_hat.transpose(1, 2)\n",
    "            loss = loss_function(y_hat, y)\n",
    "            total_loss += loss.item()\n",
    "        ppl = math.exp(total_loss / len(data_loader))\n",
    "        return ppl\n",
    "\n",
    "def remove_special_tokens(seq):\n",
    "    return [token for token in seq if token not in [BOS_TOKEN, PAD_TOKEN]]\n",
    "\n",
    "def display_audio(seq):\n",
    "    seq = remove_special_tokens(seq)\n",
    "    decode_midi(seq, \"data/music_out/tmp.mid\")\n",
    "    stream = converter.parse(\"data/music_out/tmp.mid\")\n",
    "    stream.show(\"midi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LAYERS = 2\n",
    "EMBEDDING_DIM = 64\n",
    "HIDDEN_DIM = 256\n",
    "NUM_EPOCHS = 250\n",
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 256\n",
    "CLIPPING_THRESHOLD = 1.0\n",
    "DROPOUT = 0.5\n",
    "\n",
    "LOG_EVERY_N = 1\n",
    "VAL_EVERY_N = 10\n",
    "SAVE_EVERY_N = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    collate_fn=torch.stack,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=torch.stack,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MusicRNN(\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    rnn_type=\"lstm\",\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    ").to(device)\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now().strftime(\"%Y-%m-%d_%H:%M\")\n",
    "exp_name = (\n",
    "    f\"{model.rnn_type}({EMBEDDING_DIM},{HIDDEN_DIM}),\"\n",
    "    f\"lr={LEARNING_RATE},bsz={BATCH_SIZE},nepochs={NUM_EPOCHS},\"\n",
    "    f\"sl={SEQ_LEN},dropout={DROPOUT},t={now}\"\n",
    ")\n",
    "writer = SummaryWriter(f\"runs/{exp_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in trange(1, NUM_EPOCHS + 1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        # Move to GPU if available.\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Reset gradients.\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Prepare inputs and targets.\n",
    "        x = batch[:, :-1]\n",
    "        y = batch[:, 1:]\n",
    "                \n",
    "        # Forward prop.\n",
    "        y_hat = model(x)\n",
    "        \n",
    "        # Swap token dim and output dim.\n",
    "        y_hat = y_hat.transpose(1, 2)\n",
    "        \n",
    "        # Compute loss.\n",
    "        loss = loss_function(y_hat, y)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Back prop.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip gradients.\n",
    "        clip_grad_norm_(model.parameters(), CLIPPING_THRESHOLD)\n",
    "        \n",
    "        # Update parameters.\n",
    "        optimizer.step()\n",
    "        \n",
    "    if epoch % LOG_EVERY_N == 0:\n",
    "        avg_loss = total_loss / len(train_loader)  # per token loss\n",
    "        writer.add_scalar(\"train_loss\", avg_loss, global_step=epoch)\n",
    "        \n",
    "        train_ppl = math.exp(avg_loss)\n",
    "        writer.add_scalar(\"train_ppl\", train_ppl, global_step=epoch)\n",
    "\n",
    "    if epoch % VAL_EVERY_N == 0:\n",
    "        val_ppl = validate(model, val_loader)\n",
    "        writer.add_scalar(\"val_ppl\", val_ppl, global_step=epoch)\n",
    "        generate_sequence(model, temp=1.0, topk=32);\n",
    "\n",
    "    if epoch % SAVE_EVERY_N == 0:\n",
    "        state_checkpoint = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        }\n",
    "        torch.save(state_checkpoint, f\"models/ckpt_{epoch}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating music"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's generate some music from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = model.generate_sequence(temp=1.0, topk=32)\n",
    "display_audio(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's use a priming sequence from our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIRST_N = 400\n",
    "\n",
    "primer = test_songs[-1][:FIRST_N].tolist()\n",
    "display_audio(primer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuation = model.generate_sequence(primer, temp=1.0, topk=128)\n",
    "display_audio(continuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
