{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.6 (default, Jan  8 2020, 19:59:22) \n",
      "[GCC 7.3.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from music21 import converter\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from IPython.display import display, Audio\n",
    "\n",
    "from utils import decode_midi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = \"data/pop_pickle\"\n",
    "N_SAMPLES = 909\n",
    "VOCAB_SIZE = 390\n",
    "BOS_TOKEN = VOCAB_SIZE - 2\n",
    "PAD_TOKEN = VOCAB_SIZE - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for i in range(N_SAMPLES):\n",
    "    file_name = str(i + 1).zfill(3) + \".pickle\"\n",
    "    path = os.path.join(DATA_ROOT, file_name)\n",
    "    with open(path, \"rb\") as f:\n",
    "        seq = pickle.load(f)\n",
    "        seq_tensor = torch.LongTensor(seq)\n",
    "    dataset.append(seq_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwQAAAHwCAYAAAAYZ6jEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAWJQAAFiUBSVIk8AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3de5htVX0m6u8nRBAjeAlpPTFHQEFpTaLiJUCHm0kOiheMeJrk0aBG09oS4i2RVjRomzQmGLy2niYqJJiGgEdsBYmJgCjYUSFpTyJyEXaiNoqAgtwFx/ljzorFompX1d5VtapqvO/zrGfsNecYc4411lx71bfmrVprAQAA+nSfaXcAAACYHoEAAAA6JhAAAEDHBAIAAOiYQAAAAB0TCAAAoGMCAQAAdEwgAACAjgkEAADQMYEAAAA6JhAAAEDHBAIAAOjYttPuwFpRVVcn2THJpil3BQCAjW2XJDe11naddkcSgWC2He93v/s9eM8993zwtDsCAMDGdemll+a2226bdjf+lUDwY5v23HPPB1988cXT7gcAABvYXnvtlUsuuWTTtPsxY1nOIaiqw6rqPVX1uaq6qapaVZ2yhPYfHNu0qnrUPHW2qapXVdVXquq2qrqhqs6uqn2W4zUAAECPluuk4mOSHJnk8Um+tZSGVfWsJC9JcvNm6lSSU5OckOS+Sd6b5GNJ9ktyQVU9Z8u6DQAAfVuuQPDqJHtkOCn3FYttVFU7JzkxyWlJNneszuFJDktyUZLHt9Z+r7X2W0kOTHJ3khOr6gFb2HcAAOjWsgSC1tp5rbUrWmttiU3/21i+coF6MyHjmNba7bPW+6UMYWLnDIEBAABYgqndh6CqXpTk0CQvb61dv5l62yXZJ8mtST43R5VPjeVBy91HAADY6KZylaGqekSSdyU5pbV25gLVH5VkmyRXtdbummP+FWO5xyLXPd+hSY9ZTHsAANhIVn0PQVXdJ8nJGU4iPmoRTXYayxvnmT8z/YFb2TUAAOjONPYQvDrJ/kkOaa19bxmWV2O5qPMXWmt7zbmQYc/BE5ehPwAAsG6s6h6Cqto9yR8m+XBr7exFNpvZA7DTPPN3nKgHAAAs0mofMvTYJNslefGsG5G1qmoZ9hokyRXjtEPH51dmuLToblU11x6N3cfy8hXtOQAAbECrfcjQpiQfnGfeIUkemuT0JDeNddNau6OqLkryS+PjvIl2Tx/Lc5e5rwAAsOGtaiBorf1DkpfONa+qzs8QCN7QWrtyYvb7M4SBt1XV02buRVBVT07y75N8N8lHV6rfAACwUS1LIBgP75k5xOehY7l3VZ00/vu61trrtmIVpyb5tQw3H/v7qvpEkodkCAPbJHlZa+2mrVg+AAB0abn2EDw+yRET03YbH0nyz0m2OBC01lpV/XqSi5K8JMnvJLk9yQVJ3tZau2hLlw0AAD1blkDQWjs2ybFbuYwDFph/V5ITxgcAALAMVv3GZAAAwNohEAAAQMcEAgAA6Nhq34cAYEPb5eizVnT5m447ZEWXD0B/7CEAAICOCQQAANAxgQAAADomEAAAQMcEAgAA6JhAAAAAHRMIAACgYwIBAAB0TCAAAICOCQQAANAxgQAAADomEAAAQMcEAgAA6JhAAAAAHRMIAACgYwIBAAB0TCAAAICOCQQAANAxgQAAADomEAAAQMcEAgAA6JhAAAAAHRMIAACgYwIBAAB0TCAAAICOCQQAANAxgQAAADomEAAAQMcEAgAA6JhAAAAAHRMIAACgYwIBAAB0TCAAAICObTvtDgCsll2OPmvaXQCANcceAgAA6JhAAAAAHRMIAACgYwIBAAB0TCAAAICOCQQAANAxgQAAADomEAAAQMeWJRBU1WFV9Z6q+lxV3VRVrapOmafu7lX1+qo6t6q+UVV3VtV3qurjVXXgAus5oqq+WFU3V9WNVXV+VT1zOV4DAAD0aLn2EByT5Mgkj0/yrQXq/uckxyX5N0nOTvKOJBcmOSTJuVV11FyNqur4JCcleViSE5OckuTnknyiqo7c+pcAAAD92XaZlvPqJN9McmWS/ZOct5m65yR5e2vt72dPrKr9k/xNkj+pqtNba9fMmrdPktcm+XqSJ7fWvjdO/5MkFyc5vqo+2VrbtEyvBwAAurAsewhaa+e11q5orbVF1D1pMgyM0z+b5Pwk902yz8Tsl4/lH86EgbHNpiTvS7JdkhdvWe8BAKBfa+2k4h+O5V0T0w8ay3PmaPOpiToAAMAiLdchQ1utqh6R5GlJbk1ywazp90/yM0lunn0Y0SxXjOUei1zPxfPMesziewsAABvDmggEVbVdko9kOPTn92cfFpRkp7G8cZ7mM9MfuELdAwCADWvqgaCqtknyF0n2TXJakuO3cFELnr+QJK21vebpx8VJnriF6wYAgHVpqucQjGHglCTPT/JXSV4wx4nJM3sAdsrcFtqDAAAAzGNqgaCqtk3y35McnuQvk/xGa23yZOK01m7JcG+Dn6yqh82xqN3H8vKV6isAAGxUUwkEVXXfJGdk2DPw50le2Fq7ezNNzh3Lg+eY9/SJOgAAwCKteiAYTyD+WJLnJPlgkhe31n60QLMPjOUbq+pBs5a1S5JXJrkjyYeXvbMAALDBLctJxVV1aJJDx6cPHcu9q+qk8d/XtdZeN/77A0mekeS6DIcCvbmqJhd5fmvt/JknrbWLqupPk7wmyVeq6owMNzD790kenOR33KUYAACWbrmuMvT4JEdMTNttfCTJPyeZCQS7juVPJXnzZpZ5/uwnrbXXVtVXkhyZ5LeT/CjJJUn+pLX2yS3uOQAAdGxZAkFr7dgkxy6y7gFbsZ6Tk5y8pe0BAIB7muplRwEAgOkSCAAAoGMCAQAAdGy5TioGYBXscvRZ0+7CVtt03CHT7gIAs9hDAAAAHRMIAACgYwIBAAB0TCAAAICOCQQAANAxgQAAADomEAAAQMcEAgAA6JhAAAAAHRMIAACgYwIBAAB0TCAAAICOCQQAANAxgQAAADomEAAAQMcEAgAA6JhAAAAAHRMIAACgYwIBAAB0TCAAAICOCQQAANAxgQAAADomEAAAQMcEAgAA6JhAAAAAHRMIAACgYwIBAAB0TCAAAICOCQQAANAxgQAAADomEAAAQMcEAgAA6JhAAAAAHRMIAACgYwIBAAB0TCAAAICOCQQAANAxgQAAADomEAAAQMcEAgAA6JhAAAAAHRMIAACgY9sux0Kq6rAk+yd5fJJfSPKAJB9prb1gM232SXJMkl9Msn2SK5N8KMl7Wmt3z9PmmUlel+QJSbZJ8k9J/mtr7eTleB3AdO1y9FnT7gIAdGdZAkGGP+x/IcnNSb6Z5DGbq1xVz0ny0SS3JzktyQ1JnpXkhCT7Jnn+HG2OTPKeJNcnOSXJnUkOS3JSVf1ca+11y/RaAACgG8t1yNCrk+yRZMckr9hcxaraMcmJSe5OckBr7bdaa7+XYe/CF5IcVlWHT7TZJcnxGYLDk1prr2ytvTrJzyf5epLXVtXey/RaAACgG8sSCFpr57XWrmittUVUPyzJzklOba19edYybs+wpyG5d6h4SZLtkry3tbZpVpvvJfmj8enLt7D7AADQrWmcVHzQWJ4zx7wLktyaZJ+q2m6RbT41UQcAAFik5TqHYCkePZaXT85ord1VVVcneWyS3ZJcuog211TVLUkeXlU7tNZu3dzKq+rieWZt9rwHAADYiKaxh2Cnsbxxnvkz0x+4BW12mmc+AAAwh2nsIVhIjeVizkdYcpvW2l5zLmDYc/DEJawTAADWvWnsIVjo1/wdJ+otpc1NW9EvAADozjQCwWVjucfkjKraNsmuSe5KctUi2zwsyf2TfHOh8wcAAIB7mkYgOHcsD55j3n5JdkhyUWvtjkW2efpEHQAAYJGmEQjOSHJdksOr6kkzE6tq+yRvG5++f6LNh5PckeTI8SZlM20elOQN49MPrFB/AQBgw1qWk4qr6tAkh45PHzqWe1fVSeO/r2utvS5JWms3VdXLMgSD86vq1Ax3IH52hsuLnpHktNnLb61dXVW/l+TdSb5cVacluTPDTc4enuQdrbUvLMdrAQCAnizXVYYen+SIiWm7jY8k+eckr5uZ0Vo7s6r2T/LGJM9Lsn2SK5O8Jsm757rjcWvtPVW1aVzOb2bYu/HVJMe01k5eptcBAABdWZZA0Fo7NsmxS2xzYZJnLLHNJ5J8YiltAACA+U3jHAIAAGCNEAgAAKBjAgEAAHRMIAAAgI4JBAAA0DGBAAAAOiYQAABAxwQCAADomEAAAAAdEwgAAKBjAgEAAHRMIAAAgI4JBAAA0DGBAAAAOiYQAABAxwQCAADomEAAAAAdEwgAAKBjAgEAAHRMIAAAgI4JBAAA0DGBAAAAOiYQAABAxwQCAADomEAAAAAdEwgAAKBjAgEAAHRMIAAAgI4JBAAA0DGBAAAAOiYQAABAxwQCAADomEAAAAAdEwgAAKBjAgEAAHRMIAAAgI4JBAAA0DGBAAAAOiYQAABAxwQCAADomEAAAAAdEwgAAKBjAgEAAHRMIAAAgI4JBAAA0DGBAAAAOjbVQFBVh1TVp6vqm1V1W1VdVVWnV9Xe89Tfp6rOrqobqurWqvpKVb2qqrZZ7b4DAMBGMLVAUFVvT/LJJE9Mck6SdyW5JMlzklxYVS+YqP+cJBck2S/Jx5K8L8l9k5yQ5NTV6zkAAGwc205jpVX10CSvS/KdJD/fWrt21rwDk5yb5K1JThmn7ZjkxCR3JzmgtfblcfqbxrqHVdXhrTXBAAAAlmBaewgeMa7772aHgSRprZ2X5AdJdp41+bDx+akzYWCse3uSY8anr1jRHgMAwAY0rUBwRZI7kzylqn5q9oyq2i/JA5L87azJB43lOXMs64IktybZp6q2W4G+AgDAhjWVQ4ZaazdU1euT/GmSr1bVmUmuT/LIJM9O8jdJ/sOsJo8ey8vnWNZdVXV1kscm2S3JpZtbd1VdPM+sxyzpRQAAwAYwlUCQJK21d1bVpiQfSvKyWbOuTHLSxKFEO43ljfMsbmb6A5e1kwAAsMFN8ypDv5/kjCQnZdgzcP8keyW5KslHquqPl7K4sWwLVWyt7TXXI8nXlvQCAABgA5hKIKiqA5K8Pcn/aK29prV2VWvt1tbaJUmem+RbSV5bVbuNTWb2AOx076UlSXacqAcAACzCtA4ZeuZYnjc5o7V2a1V9MUMweEKGPQaXJXlSkj2S3OMcgKraNsmuSe4a6wKwhu1y9FkruvxNxx2yossH2GimdcjQzNWAdp5n/sz0O8fy3LE8eI66+yXZIclFrbU7lqd7AADQh2kFgs+N5W9X1c/MnlFVT0+yb5Lbk1w0Tj4jyXVJDq+qJ82qu32St41P37+iPQYAgA1oWocMnZHhPgO/nOTSqvpYkm8n2TPD4USV5OjW2vVJ0lq7qapeNrY7v6pOTXJDhkuUPnqcftqqvwoAAFjnpnUfgh9V1TOSvDLJ4RnOF9ghwx/5Zyd5d2vt0xNtzqyq/ZO8Mcnzkmyf4RKlrxnrL3iFIQAA4J6meR+CHyZ55/hYbJsLkzxjxToFAACdmdp9CAAAgOkTCAAAoGMCAQAAdEwgAACAjgkEAADQMYEAAAA6JhAAAEDHBAIAAOiYQAAAAB0TCAAAoGMCAQAAdEwgAACAjgkEAADQMYEAAAA6JhAAAEDHBAIAAOiYQAAAAB0TCAAAoGMCAQAAdEwgAACAjgkEAADQMYEAAAA6JhAAAEDHBAIAAOiYQAAAAB0TCAAAoGMCAQAAdEwgAACAjgkEAADQMYEAAAA6JhAAAEDHBAIAAOiYQAAAAB0TCAAAoGMCAQAAdEwgAACAjgkEAADQMYEAAAA6JhAAAEDHBAIAAOiYQAAAAB0TCAAAoGMCAQAAdEwgAACAjgkEAADQMYEAAAA6NvVAUFW/VFUfraprquqOsfx0VT1jjrr7VNXZVXVDVd1aVV+pqldV1TbT6DsAAKx3205z5VV1TJL/nOS6JJ9Mck2Sn0ryhCQHJDl7Vt3nJPloktuTnJbkhiTPSnJCkn2TPH8Vuw4AABvC1AJBVT0/Qxj42yS/1lr7wcT8n5j17x2TnJjk7iQHtNa+PE5/U5JzkxxWVYe31k5drf4DAMBGMJVDhqrqPknenuTWJL8xGQaSpLX2w1lPD0uyc5JTZ8LAWOf2JMeMT1+xcj0GAICNaVp7CPZJsmuSM5J8r6oOSfK4DIcDfbG19oWJ+geN5TlzLOuCDMFin6rarrV2xwr1GQAANpxpBYInj+V3klyS5Odmz6yqC5Ic1lr77jjp0WN5+eSCWmt3VdXVSR6bZLckl25uxVV18TyzHrO4rgMAwMYxrasM/fRYvjzJ/ZL8cpIHZNhL8NdJ9kty+qz6O43ljfMsb2b6A5e3mwAAsLFNaw/BzGVCK8OegP81Pv+nqnpuhj0B+1fV3nMcPjSXGsu2UMXW2l5zLmDYc/DERawLAAA2jGntIfjeWF41KwwkSVprt2XYS5AkTxnLmT0AO2VuO07UAwAAFmFageCysfz+PPNnAsP9JurvMVmxqrbNcILyXUmuWq4OAgBAD6YVCC7I8Af87lV13znmP24sN43luWN58Bx190uyQ5KLXGEIAACWZiqBoLV2XYa7De+U5M2z51XVryT5vzIc/jNzmdEzMtzN+PCqetKsutsnedv49P0r3G0AANhwpnan4iSvSfLUJG+sqv2SfDHJI5I8N8MdiV/WWvt+krTWbqqql2UIBudX1alJbkjy7AyXJD0jQ8AAVtAuR5817S4AAMtsWocMpbV2bYZAcEKSn01yVIYbkJ2V5Jdaa6dP1D8zyf4ZDjd6XpLfSfLDDMHi8NbaglcYAgAA7mmaewjSWrshwx/0r1lk/QuTPGNFOwUAAB2Z2h4CAABg+gQCAADomEAAAAAdEwgAAKBjAgEAAHRMIAAAgI4JBAAA0DGBAAAAOiYQAABAxwQCAADomEAAAAAdEwgAAKBjAgEAAHRMIAAAgI4JBAAA0DGBAAAAOiYQAABAxwQCAADomEAAAAAdEwgAAKBjAgEAAHRs22l3AACW0y5Hn7Xi69h03CErvg6A1WIPAQAAdEwgAACAjgkEAADQMYEAAAA6JhAAAEDHBAIAAOiYQAAAAB0TCAAAoGMCAQAAdEwgAACAjgkEAADQMYEAAAA6JhAAAEDHBAIAAOiYQAAAAB0TCAAAoGMCAQAAdEwgAACAjgkEAADQMYEAAAA6JhAAAEDHBAIAAOiYQAAAAB0TCAAAoGNrJhBU1Qurqo2Pl85T55lVdX5V3VhVN1fV31XVEavdVwAA2CjWRCCoqp9N8p4kN2+mzpFJPpHkcUlOSXJikv8jyUlVdfxq9BMAADaaqQeCqqokH05yfZIPzFNnlyTHJ7khyZNaa69srb06yc8n+XqS11bV3qvSYQAA2ECmHgiSHJXkoCQvTnLLPHVekmS7JO9trW2amdha+16SPxqfvnwF+wgAABvSVANBVe2Z5Lgk72qtXbCZqgeN5TlzzPvURB0AAGCRtp3Wiqtq2yR/keRfkrxhgeqPHsvLJ2e01q6pqluSPLyqdmit3brAei+eZ9ZjFugDAABsOFMLBEnenOQJSf5da+22BeruNJY3zjP/xiT3H+ttNhAAAAA/NpVAUFVPybBX4B2ttS8sxyLHsi1UsbW21zx9ujjJE5ehLwAAsG6s+jkEsw4VujzJmxbZbGbPwE7zzN9xLG/aiq4BAEB3pnFS8U8m2SPJnklun3UzspbkD8Y6J47T3jk+v2ws95hcWFU9LMPhQt9c6PwBAADgnqZxyNAdST44z7wnZjiv4PMZQsDM4UTnJtk3ycGzps14+qw6AADAEqx6IBhPIH7pXPOq6tgMgeDk1tqfzZr14SS/n+TIqvrwzL0IqupB+fEViua8qRkAADC/aV5laNFaa1dX1e8leXeSL1fVaUnuTHJYkodn+U5OBgCArqyLQJAkrbX3VNWmJK9L8psZzn/4apJjWmsnT7NvAACwXq2pQNBaOzbJsZuZ/4kkn1it/gAAwEY3jasMAQAAa4RAAAAAHRMIAACgYwIBAAB0TCAAAICOCQQAANAxgQAAADomEAAAQMcEAgAA6JhAAAAAHRMIAACgYwIBAAB0TCAAAICOCQQAANAxgQAAADomEAAAQMcEAgAA6JhAAAAAHRMIAACgYwIBAAB0TCAAAICOCQQAANAxgQAAADomEAAAQMcEAgAA6JhAAAAAHRMIAACgY9tOuwPQi12OPmtFl7/puENWdPkAwMZkDwEAAHRMIAAAgI4JBAAA0DGBAAAAOiYQAABAx1xlCDaIlb6KEQCwMdlDAAAAHRMIAACgYwIBAAB0TCAAAICOCQQAANAxgQAAADrmsqMAsEQrfZnfTccdsqLLB5jNHgIAAOiYQAAAAB0TCAAAoGMCAQAAdEwgAACAjk0lEFTVQ6rqpVX1saq6sqpuq6obq+rzVfVbVTVnv6pqn6o6u6puqKpbq+orVfWqqtpmtV8DAABsBNO67Ojzk7w/yTVJzkvyL0n+TZJfS/JnSZ5eVc9vrbWZBlX1nCQfTXJ7ktOS3JDkWUlOSLLvuEwAAGAJphUILk/y7CRntdZ+NDOxqt6Q5ItJnpchHHx0nL5jkhOT3J3kgNbal8fpb0pybpLDqurw1tqpq/oqAABgnZvKIUOttXNba5+YHQbG6d9O8oHx6QGzZh2WZOckp86EgbH+7UmOGZ++YuV6DAAAG9NaPKn4h2N516xpB43lOXPUvyDJrUn2qartVrJjAACw0UzrkKE5VdW2SX5zfDr7j/9Hj+Xlk21aa3dV1dVJHptktySXLrCOi+eZ9Zil9RYAANa/tbaH4Lgkj0tydmvtr2dN32ksb5yn3cz0B65UxwAAYCNaM3sIquqoJK9N8rUkL1xq87Fsm62VpLW21zzrvzjJE5e4XgAAWNfWxB6Cqnplkncl+WqSA1trN0xUmdkDsFPmtuNEPQAAYBGmHgiq6lVJ3pvkHzOEgW/PUe2ysdxjjvbbJtk1w0nIV61UPwEAYCOaaiCoqtdnuLHYP2QIA9fOU/XcsTx4jnn7JdkhyUWttTuWv5cAALBxTS0QjDcVOy7JxUme1lq7bjPVz0hyXZLDq+pJs5axfZK3jU/fv1J9BQCAjWoqJxVX1RFJ3prhzsOfS3JUVU1W29RaOylJWms3VdXLMgSD86vq1CQ3ZLjb8aPH6aetTu8BAGDjmNZVhnYdy22SvGqeOp9NctLMk9bamVW1f5I3Jnleku2TXJnkNUne3Vpb8ApDAADAPU0lELTWjk1y7Ba0uzDJM5a7PwAA0KupX2UIAACYHoEAAAA6JhAAAEDHBAIAAOiYQAAAAB0TCAAAoGMCAQAAdEwgAACAjgkEAADQsancqRgAmJ5djj5rxdex6bhDVnwdwPKwhwAAADomEAAAQMcEAgAA6JhAAAAAHRMIAACgYwIBAAB0TCAAAICOCQQAANAxNyYDgDVmNW4cBjDDHgIAAOiYQAAAAB0TCAAAoGMCAQAAdEwgAACAjgkEAADQMYEAAAA6JhAAAEDHBAIAAOiYQAAAAB0TCAAAoGPbTrsDsFbscvRZ0+4CAMCqs4cAAAA6JhAAAEDHBAIAAOiYQAAAAB0TCAAAoGOuMgQAMGE1rjy36bhDVnwdsBj2EAAAQMcEAgAA6JhDhgCAZbfSh9w43AaWjz0EAADQMYEAAAA6JhAAAEDHBAIAAOiYQAAAAB0TCAAAoGPr6rKjVfXwJG9NcnCShyS5JsmZSd7SWvveNPu21rn8GwAbyWrcSZjp8/fL6lg3gaCqHpnkoiQ/neTjSb6W5ClJfjfJwVW1b2vt+il2EQAA1p31dMjQf80QBo5qrR3aWju6tXZQkhOSPDrJH061dwAAsA6tiz0EVbVbkl9NsinJ+yZm/0GS307ywqp6bWvtllXu3laz23NhxggAYGWslz0EB43lp1trP5o9o7X2gyQXJtkhyS+udscAAGA9Wxd7CDIcEpQkl88z/4oMexD2SPKZzS2oqi6eZ9YvXHrppdlrr722rIdb4Zpv3bjq61xue/3Nm1d0+RthjABgtpX+7twIVvr7f1rvwaWXXpoku0xl5XNYL4Fgp7Gcb6uYmf7ArVjH3bfddtuNl1xyyaatWEa3LvlOkuQx49OvTa8nG4JxXD7GcvkYy+VhHJfPuh/L8btzLVj3Y7mllvk9WMo47pLkpmVd+1ZYL4FgITWWbaGKrbXV3wXQiZm9L8Z46xjH5WMsl4+xXB7GcfkYy+VjLJfHeh7H9XIOwcwegJ3mmb/jRD0AAGAR1ksguGws95hn/u5jOd85BgAAwBzWSyA4byx/taru0eeqekCSfZPcluR/rnbHAABgPVsXgaC19vUkn85wAsYrJ2a/Jcn9k/z5erwHAQAATNN6Oqn4Pya5KMm7q+ppSS5N8tQkB2Y4VOiNU+wbAACsS9XaghfmWTOq6meTvDXJwUkekuSaJGcmeUtr7YZp9g0AANajdRUIAACA5bUuziEAAABWhkAAAAAdEwgAAKBjAgEAAHRMIAAAgI4JBAAA0DGBoDNV9ZCqemlVfayqrqyq26rqxqr6fFX9VlXNuU1U1T5VdXZV3VBVt1bVV6rqVVW1zWbW9cyqOn9c/s1V9XdVdcQC/Tuiqr441r9xbP/MrX3dq6WqXlhVbXy8dJ46Kz4uVbXN+P58ZXyPbxjfv3229jWupKr6par6aFVdU1V3jOWnq+oZc9S1Tc6hqg4Zx+yb43t/VVWdXlV7z1O/63GsqsOq6j1V9bmqumn87J6yQJs1OWbT/NwvZRyraveqen1VnVtV36iqO6vqO1X18ao6cIH1rPiYVNX9quotVXVZVd1eVddW1V9V1Z6LH5EttyXb5ET7D9aPv4ceNU+dVRmXqnpwVb2zqjbV8H/6/66qD1XVwxf7erbGFn6+a9zOzh/H5baqunp8rXvM02b9b5etNY+OHklenqQl+d9JPpLkvyT5UJLvj9PPyHh/illtnpPkriQ3J/lgkj9J8rWx/unzrOfIcf51Sd6X5IQk3xinHT9Pm+PH+d8Y678vyfXjtCOnPXaLGNufHcfxB2OfXzqNcUlSSU4f539tfL8+OL5/dyzrE6EAAAsBSURBVCV5zrTHap7XeczY5+8m+XCSP0ry35J8Kckf2yYXNYZvn/Ua/yzJceNn+s4kP0ryAuN4r779w9iPHyS5dPz3KZupvybHbNqf+6WMY5JTx/n/lOT/yfA99P+O/WxJjprWmCTZLsnnxzZfGj9Tf5nkh0luSfLUtbZNTrR91qy2LcmjpjUuGW4ge9nY5jMZ/j86c3z+nSS7rbWxTLJ9kk/MGpv3jtvnyUmuSvLMjbpdrugb4bH2HkkOGv/DuM/E9Icm+ZdxY3verOk7Jrk2yR1JnjRr+vZJLhrrHz6xrF2S3D5+IHaZNf1BSa4c2+w90WafcfqVSR40sazrx+XtsjWvfYXHtZL8bZKvjx/uewWC1RqXJL8+trkwyfazpj95fB+vTfKAaY/ZRJ+fP/b5b+bqW5KfsE0uOIYPTXJ3km8n+emJeQeOr+Uq43ivcTswye7jZ/iAbP4P2TU7Zpny536J4/iiJE+YY/r+GcLrHUkeNo0xSfKfxjanZ9b3ZIYgOBNi7rPQeKzWWE602znD5//UJOdn/kCwKuOSIey1JH86Mf2ocfo5KzmOWzKWGf6Ybxl+kLrX+5xZ30Ubbbtc0TfCY309krxh3LDeM2vaS8ZpJ89R/6Bx3mcnpr91nP6WOdrMubwkfz5Of/EcbeZd3lp5JPndDL/A7pfk2MwdCFZlXJJcME4/cI428y5vimN3nwy/vNySZOdF1LdNzj0uTx379PF55t+U5AfGcbNjeEA2/4fsmh2ztfS5X2gcF2j76Uz8MLVaY5Lhj8Z/HqfvOkebeZe3FsYyyccyBIKHZPOBYMXHJcn9k9ya4VfvyT9u75Pk6rHNiu8lWOxYJnlkhh9VvpiJIyU2s8wNs106h4DZfjiWd82adtBYnjNH/QsyfOD3qartFtnmUxN1tqbNmjAev3dckne11i7YTNUVH5fxfdgnw/vyuSWsZ5r2SbJrkrOTfK+GY+BfX1W/W3Mf926bnNsVGX5dfUpV/dTsGVW1X5IHZNiLNcM4Lt2aHLN1+rmfz1zfQ8nqjMkjk/yfSS5vrV29yDZrQlW9KMmhSV7eWrt+M/VWa1z2TnK/JBe21n4wu3Jr7UcZgl8y/IK/Vvx6hrBycpIdq+oFVfWfquq35zsXIxtou9x2axqzcVTVtkl+c3w6e8N+9FhePtmmtXZXVV2d5LFJdstwfN5Cba6pqluSPLyqdmit3VpV90/yM0lubq1dM0f3rhjLOU/mmaZx3P4iw+FWb1ig+mqMy6OSbJPh0JDJL9T52kzbk8fyO0kuSfJzs2dW1QVJDmutfXecZJucQ2vthqp6fZI/TfLVqjozwy7rRyZ5dobDsf7DrCbGcenW6pitx8/9vVTVI5I8LcMfSxfMmr5aYzLve7WZNlM3jtu7MvzyfeYC1VdrXNbjWM58F+2U4fDfh8ya16rq/RnOb7k72XjbpT0EzDguyeOSnN1a++tZ03cayxvnaTcz/YFb0GaniXIp61gr3pzkCUle1Fq7bYG6qzEu63Esf3osX57hF6VfzvBr9uOS/HWGw7BOn1XfNjmP1to7k/xahh97Xpbk6AznZ3wjyUmttWtnVTeOS7dWx2zdj/P4y+lHMpw8eWxr7XuzZq/WmKy7cazhyoAnZzg056hFNDGW85v5Lnprki9n+HHqARlC6teT/Mckb5pVf0ONpUBAquqoJK/NcLb7C5fafCzbCrfZkvorqqqekmGvwDtaa19YjkWO5UqOy5auYyXNXKqxMuwJ+Exr7ebW2j8leW6SbybZf57Dh+bS8zb5+xmuKnRShj0D90+yV4ZzND5SVX+8lMWNZXfjuBXW6pitxc/9v6rhcq1/kWTfJKdluGrLlljpMVmL4/jqDCdjv2wiRG2p1RqXtTiWM99F1yR5bmvtH8fvonOTHJbhPMHXVNV9l7jcdTGWAkHnquqVGXY1fjXDCSk3TFSZ/PVq0o4T9ZbS5qZF1l8oHa+6WYcKXZ57/mKwOasxLlvyfk3bzJfYVa21/zV7xrjXZWaP1VPG0jY5h6o6IMOl6P5Ha+01rbWrWmu3ttYuyRCsvpXktVW129jEOC7dWh2z9fi5T/KvYeCUDHuy/irDpXEn/7BZrTFZV+NYVbsn+cMkH26tnb3IZqs1LutqLEcz30XnTO7xH7+brs6wx2Dmuv8barsUCDpWVa/KcI3df8wQBr49R7XLxvJex6aNfxTvmuHkr6sW2eZhGX61/GZr7dYkaa3dkuGPlZ8c50/afSznO35uGn4yw+vbM8nt9eObwLQkfzDWOXGc9s7x+WqMy5UZrpKw2/j+LKbNtM2My/fnmT/zn/T9JurbJu9p5iY4503OGF/XFzP8n/+EcbJxXLq1Ombr8XM/M2b/PcnhGa6p/htzHVe9imMy73u1mTbT9NgMh1i9ePZ30Pg9tP9Y54px2qHj89Ual/U2lskSv4s22nYpEHRqPPnwhAw37Thw4tji2c4dy4PnmLdfkh2SXNRau2ORbZ4+UWdr2kzTHRluJDLX4+/HOp8fn88cTrTi4zK+DxdleF9+aQnrmaYLMvwRtfs8u2IfN5abxtI2ObeZK9vsPM/8mel3jqVxXLo1OWbr8XM/ftbPyLBn4M+TvHDmZM15rMaYfD3DBSL2qKpdF9lmmjZl/u+hmR/4Th+fb0pWdVz+Z5LbkuxbVQ+YXXk87+FXx6f3+gFjij4zlo+bnDGe4zLzh/emWbM2zna5Ndcs9VifjwyHuLQMJ808eIG6O2a4c+xSbsSzazbYzYuWOL7HZu77EKzKuGRxNz3ZcdrjNNHnU8Y+v21i+q9kOG7z+0keaJvc7Bj+32N/v53kZybmPX0cx9uSPMQ4zjuGB2Tz1ylfs2O2lj73ixjH7ZKcNdb5syzihkqrNSZZAzcmW8pYbqbd+Zn/PgSrMi758Y3J3jExfdVuTLbE7fK+Gf74/lGSX5mY97ax7fkbdbtctTfCY208khwxbjx3ZdhDcOwcjxdNtDl0rH9zhv+8/zjDCcgzG+e9buCR5HfG+ddluPPfCRmudNKSHD9P394xzp99++/rxmn3uv33Wn1knkCwWuOSe94W/dLx/drsbdGn/chwdYcrxj5fkOGkwtPH/v4wyfNtkwuO4X0yXFq0ZTge/eSM5xRk+IJrSX7XON6rb4dmOAn7pAyXXG4Z/iiYmXb8HPXX3Jhlyp/7pYxjkg+P87+b5C2Z+3vogGmMSYawcuHY5ksZrsD3lxn+H7olyVPX2jY5zzLOz/yBYFXGJcNlOy8b23wmyX9Jcub4/DtJHrnWxjLJv8tw2du7xjE6Pslnx3bXJtljo26XK/pGeKy9R378x+rmHufP0W7fjDeOyvAr4/+X4eoG22xmXc8aP0g/GDfYLyU5YoH+HTHWu2Vs99kkz5z2uG3hGN8rEKzWuGS47OSrx/fptvF9OzvJPtMen830+cEZrqF/dYbDWq5P8vEkvzhPfdvkvfv6E0lelWF3/U3jl8u1ST6Z5FeN45z9Wuj/xE3rZcym+blfyjjmx3+sbu5x7LTGJMMx4m/J8CPFHRmCy+lJ/u1a3SbnWMbMGN8rEKzmuGT4f/1dGe60e2eGK/h8KMnD1+pYJvm3Ga52de3Y529k2Nsxb583wnZZ40oAAIAOOakYAAA6JhAAAEDHBAIAAOiYQAAAAB0TCAAAoGMCAQAAdEwgAACAjgkEAADQMYEAAAA6JhAAAEDHBAIAAOiYQAAAAB0TCAAAoGMCAQAAdEwgAACAjgkEAADQMYEAAAA69v8DzB+3vwF6x2IAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 386
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "seq_lens = [len(seq) for seq in dataset]\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(seq_lens, bins=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 819 \t Val: 45 \t Test: 45\n"
     ]
    }
   ],
   "source": [
    "N_VAL = N_TEST = int(0.05 * 909)\n",
    "N_TRAIN = 909 - (N_VAL + N_TEST)\n",
    "\n",
    "train_songs = dataset[:N_TRAIN]\n",
    "val_songs = dataset[N_TRAIN:N_TRAIN+N_VAL]\n",
    "test_songs = dataset[N_TRAIN+N_VAL:]\n",
    "\n",
    "print(f\"Train: {len(train_songs)} \\t Val: {len(val_songs)} \\t Test: {len(test_songs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(seq, new_seq_len):\n",
    "    \"\"\"\n",
    "    Convert a sequence into a batch of equal-length sequences,\n",
    "    and prepend a BOS token at the start of every sequence.\n",
    "    \n",
    "    Unless new_seq_len perfectly divides the number of tokens\n",
    "    in seq, the last sequence will be padded with PAD_TOKENs.\n",
    "    \"\"\"\n",
    "    n = seq.numel() % new_seq_len\n",
    "    pad_size = 0 if n == 0 else new_seq_len - n\n",
    "    pad = torch.full(size=(pad_size,), fill_value=PAD_TOKEN)\n",
    "    seq = torch.cat([seq, pad])\n",
    "    assert seq.numel() % new_seq_len == 0\n",
    "    num_batches = int(seq.numel() / new_seq_len)\n",
    "    batch = seq.view(num_batches, -1)\n",
    "    bos = torch.full(size=(num_batches, 1), fill_value=BOS_TOKEN)\n",
    "    batch = torch.cat([bos, batch], dim=1)    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 6166 \t Val: 355 \t Test: 336\n"
     ]
    }
   ],
   "source": [
    "SEQ_LEN = 1024\n",
    "\n",
    "train_data = torch.cat([batchify(seq, SEQ_LEN) for seq in train_songs])\n",
    "val_data = torch.cat([batchify(seq, SEQ_LEN) for seq in val_songs])\n",
    "test_data = torch.cat([batchify(seq, SEQ_LEN) for seq in test_songs])\n",
    "\n",
    "print(f\"Train: {len(train_data)} \\t Val: {len(val_data)} \\t Test: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicRNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, rnn_type=\"lstm\", num_layers=1, dropout=0.0):\n",
    "        assert rnn_type in [\"lstm\", \"gru\"]\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.rnn_type = rnn_type\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        if rnn_type == \"lstm\":\n",
    "            self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers, dropout=dropout, batch_first=True) \n",
    "        else:\n",
    "            self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "        self.hidden_state = None\n",
    "\n",
    "    def forward(self, x, reset_hidden_state=True):\n",
    "        x = self.embeddings(x)\n",
    "        if reset_hidden_state:\n",
    "            x, _ = self.rnn(x)\n",
    "        else:\n",
    "            x, self.hidden_state = self.rnn(x, self.hidden_state)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def generate_sequence(self, beam_width=1, seq_start=None, max_length=1024, **kwargs):\n",
    "        if not seq_start:\n",
    "            seq_start = [BOS_TOKEN]\n",
    "        seq = seq_start.copy()\n",
    "        with torch.no_grad():\n",
    "            # Generate k most likely tokens\n",
    "            prev_top_seq, next_top_seq = [], []\n",
    "            prev_top_seq = [(seq, 0.0)]\n",
    "            \n",
    "            import pdb;\n",
    "            pdb.set_trace()\n",
    "            \n",
    "            kpos_next_tokens = self._generate_next_token(\n",
    "                candidates=torch.LongTensor(seq).to(device),\n",
    "                reset_hidden=True,\n",
    "                **kwargs\n",
    "            )\n",
    "            \n",
    "            next_top_seq.extend(kpos_next_tokens)\n",
    "            \n",
    "            while len(seq) <= max_length:\n",
    "                for sequence in prev_top_seq:\n",
    "                    kpos_next_tokens = self._generate_next_token(\n",
    "                        candidates=torch.LongTensor([kpos_next_tokens]).to(device),\n",
    "                        reset_hidden=False,\n",
    "                        **kwargs\n",
    "                    )\n",
    "                    \n",
    "                    next_top_seq.extend(kpos_next_tokens)\n",
    "                \n",
    "                next_top_seq.sort(reverse=True, key=lambda tup: tup[1])\n",
    "                prev_top_seq = next_top_seq[:self.beam_width]\n",
    "                next_top_seq = []\n",
    "                \n",
    "                seq.append(next_token)\n",
    "        \n",
    "        n = min(len(prev_top_seq), max_length)\n",
    "        return prev_top_seq[:n]\n",
    "    \n",
    "    def _generate_next_token(self, candidates, reset_hidden=False, temp=1.0, topk=5, argmax=False):\n",
    "        # The model expects a batch input, so we add a fake batch dimension.\n",
    "        # Removed the fake batch dimension (.unsqueeze(0)) because we have batch of beam_width tokens\n",
    "        model_input = np.array([tup[0] for tup in candidates]) # Previous sequences: [(seq , score), (seq2, score), (seq3, score)]\n",
    "        # Then, we need to remove the fake batch dimension from the output.\n",
    "        # Also removed the (.squeeze(0)) for similar reasons\n",
    "        model_output = self(model_input, reset_hidden)\n",
    "        \n",
    "        # Apply softmax to top beam_width tokens\n",
    "        \n",
    "        for i in range(len(candidates)):\n",
    "            next_token_probs = [(candidates[i].first + token, candidates[i].second + np.log(score)) for score in F.softmax(model_output[:, i] / temp, dim=0)]\n",
    "        \n",
    "        if argmax:\n",
    "            # Keep top beam_width tokens\n",
    "            kpos_next_tokens = torch.topk(next_token_probs, self.beam_width)\n",
    "        else:\n",
    "            # TO-DO: Implement beam search for this case too\n",
    "            top_tokens = torch.topk(next_token_probs, topk)\n",
    "            top_indices = top_tokens.indices\n",
    "            top_probs = top_tokens.values\n",
    "            top_probs /= torch.sum(top_probs)\n",
    "            kpos_next_tokens = np.random.choice(top_indices.cpu().numpy(), p=top_probs.cpu().numpy())\n",
    "        return kpos_next_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, data_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0.0\n",
    "        for batch in data_loader:\n",
    "            batch = batch.to(device)\n",
    "            x = batch[:, :-1]\n",
    "            y = batch[:, 1:]\n",
    "            y_hat = model(x)\n",
    "            y_hat = y_hat.transpose(1, 2)\n",
    "            loss = loss_function(y_hat, y)\n",
    "            total_loss += loss.item()\n",
    "        ppl = math.exp(total_loss / len(data_loader))\n",
    "        return ppl\n",
    "\n",
    "def remove_special_tokens(seq):\n",
    "    return [token for token in seq if token not in [BOS_TOKEN, PAD_TOKEN]]\n",
    "\n",
    "def display_audio(seq):\n",
    "    seq = remove_special_tokens(seq)\n",
    "    decode_midi(seq, \"data/music_out/tmp.mid\")\n",
    "    stream = converter.parse(\"data/music_out/tmp.mid\")\n",
    "    stream.show(\"midi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LAYERS = 1\n",
    "EMBEDDING_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "NUM_EPOCHS = 250\n",
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 64\n",
    "CLIPPING_THRESHOLD = 1.0\n",
    "DROPOUT = 0.0\n",
    "\n",
    "LOG_EVERY_N = 1\n",
    "VAL_EVERY_N = 10\n",
    "SAVE_EVERY_N = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    collate_fn=torch.stack,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=torch.stack,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MusicRNN(\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    rnn_type=\"lstm\",\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    ").to(device)\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MusicRNN(\n",
      "  (embeddings): Embedding(390, 256)\n",
      "  (rnn): LSTM(256, 512, batch_first=True)\n",
      "  (fc): Linear(in_features=512, out_features=390, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now().strftime(\"%Y-%m-%d_%H:%M\")\n",
    "exp_name = (\n",
    "    f\"{model.rnn_type}({EMBEDDING_DIM},{HIDDEN_DIM}),\"\n",
    "    f\"lr={LEARNING_RATE},bsz={BATCH_SIZE},nepochs={NUM_EPOCHS},\"\n",
    "    f\"sl={SEQ_LEN},dropout={DROPOUT},t={now}\"\n",
    ")\n",
    "writer = SummaryWriter(f\"runs/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "387aab8a3664434594c63067de42d3dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=97.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "576ded5e1f8640d2b84eb3b82fff7932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=97.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ed23abe4fa14b3588e8e45af9c4c5f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=97.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "554fe160a53a4798a6b6796e21e3b313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=97.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12ee1c116b17464681ddd470ebfc5326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=97.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "206fa124f33e4581aa409cfd29bff1e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=97.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d8ec01882ab4b21823c0ba4eab5f6b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=97.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d8934128c045e3a55b0957add3b503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=97.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e667f40afc24356bd1925c2366f545f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=97.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-9c011755bf3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Compute loss.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# Back prop.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in tqdm(train_loader):\n",
    "        # Move to GPU if available.\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Reset gradients.\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Prepare inputs and targets.\n",
    "        x = batch[:, :-1]\n",
    "        y = batch[:, 1:]\n",
    "                \n",
    "        # Forward prop.\n",
    "        y_hat = model(x)\n",
    "        \n",
    "        # Swap token dim and output dim.\n",
    "        y_hat = y_hat.transpose(1, 2)\n",
    "        \n",
    "        # Compute loss.\n",
    "        loss = loss_function(y_hat, y)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Back prop.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip gradients.\n",
    "        clip_grad_norm_(model.parameters(), CLIPPING_THRESHOLD)\n",
    "        \n",
    "        # Update parameters.\n",
    "        optimizer.step()\n",
    "        \n",
    "    if epoch % LOG_EVERY_N == 0:\n",
    "        avg_loss = total_loss / len(train_loader)  # per token loss\n",
    "        writer.add_scalar(\"train_loss\", avg_loss, global_step=epoch)\n",
    "        \n",
    "        train_ppl = math.exp(avg_loss)\n",
    "        writer.add_scalar(\"train_ppl\", train_ppl, global_step=epoch)\n",
    "\n",
    "    if epoch % VAL_EVERY_N == 0:\n",
    "        val_ppl = validate(model, val_loader)\n",
    "        writer.add_scalar(\"val_ppl\", val_ppl, global_step=epoch)\n",
    "        seq = model.generate_sequence(temp=1.0, topk=32)\n",
    "        display_audio(seq)\n",
    "\n",
    "    if epoch % SAVE_EVERY_N == 0:\n",
    "        state_checkpoint = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        }\n",
    "        torch.save(state_checkpoint, f\"models/lstm_checkpoints/ckpt_{epoch}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from music_transformer import custom\n",
    "from music_transformer import utils\n",
    "from music_transformer.custom.metrics import *\n",
    "from music_transformer.custom.criterion import SmoothCrossEntropyLoss, CustomSchedule\n",
    "from music_transformer.custom.config import config\n",
    "from music_transformer.data import Data\n",
    "from music_transformer.model import MusicTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_transformer(transformer, data_loader):\n",
    "    transformer.eval()\n",
    "    val_loss = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0.0\n",
    "        for batch in data_loader:\n",
    "            batch = batch.to(device)\n",
    "            x = batch[:, :-1]\n",
    "            y = batch[:, 1:]\n",
    "            y_hat = transformer(x).transpose(1, 2)\n",
    "            loss = val_loss(y_hat, y)\n",
    "            total_loss += loss.item()\n",
    "        ppl = math.exp(total_loss / len(data_loader))\n",
    "        return ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LAYERS = 4\n",
    "EMBEDDING_DIM = 256\n",
    "NUM_EPOCHS = 1000\n",
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 8\n",
    "DROPOUT = 0.1\n",
    "LABEL_SMOOTHING = 0.1\n",
    "\n",
    "LOG_EVERY_N = 1\n",
    "VAL_EVERY_N = 2\n",
    "SAVE_EVERY_N = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    collate_fn=torch.stack,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=torch.stack,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MusicTransformer(\n",
      "  (Decoder): Encoder(\n",
      "    (embedding): Embedding(390, 256)\n",
      "    (pos_encoding): DynamicPositionEmbedding()\n",
      "    (enc_layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (rga): RelativeGlobalAttention(\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (FFN_pre): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (FFN_suf): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (layernorm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "        (layernorm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): EncoderLayer(\n",
      "        (rga): RelativeGlobalAttention(\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (FFN_pre): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (FFN_suf): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (layernorm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "        (layernorm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): EncoderLayer(\n",
      "        (rga): RelativeGlobalAttention(\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (FFN_pre): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (FFN_suf): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (layernorm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "        (layernorm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (3): EncoderLayer(\n",
      "        (rga): RelativeGlobalAttention(\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (FFN_pre): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (FFN_suf): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (layernorm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "        (layernorm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (fc): Linear(in_features=256, out_features=390, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "transformer = MusicTransformer(\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layer=NUM_LAYERS,\n",
    "    max_seq=SEQ_LEN,\n",
    "    dropout=DROPOUT,\n",
    ").to(device)\n",
    "\n",
    "print(transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = CustomSchedule(EMBEDDING_DIM, optimizer=optimizer)\n",
    "loss_function = SmoothCrossEntropyLoss(LABEL_SMOOTHING, VOCAB_SIZE, PAD_TOKEN)\n",
    "val_loss = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now().strftime(\"%Y-%m-%d_%H:%M\")\n",
    "exp_name = (\n",
    "    f\"Transformer({EMBEDDING_DIM},{VOCAB_SIZE},{NUM_LAYERS}),\"\n",
    "    f\"lr={LEARNING_RATE},bsz={BATCH_SIZE},nepochs={NUM_EPOCHS},\"\n",
    "    f\"sl={SEQ_LEN},dropout={DROPOUT},t={now}\"\n",
    ")\n",
    "writer = SummaryWriter(f\"runs/{exp_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "894ee267fb6c4307b499e93f7a5d3fa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=771.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44969bdffac34f909ed8ae88a166fc63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=771.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <div id='midiPlayerDiv242144'></div>\n",
       "                <link rel=\"stylesheet\" href=\"//cuthbertLab.github.io/music21j/css/m21.css\"\n",
       "                    type=\"text/css\" />\n",
       "                <script>\n",
       "                require.config({\n",
       "                    paths: {'music21': '//cuthbertLab.github.io/music21j/src/music21'}\n",
       "                });\n",
       "                require(['music21'], function() {\n",
       "                               mp = new music21.miditools.MidiPlayer();\n",
       "                               mp.addPlayer('#midiPlayerDiv242144');\n",
       "                               mp.base64Load('data:audio/midi;base64,TVRoZAAAAAYAAQABBABNVHJrAAAGIAD/AwAA4ABAAMAAAP9RAwehIAD/WAQEAhgIiACQMFQAkDxYAJBAWACQQ2SCAIAwAIMrgEAAAIBDAACQMFgAkENcAJBMWIIAgDAAAIBDAFWAPAAAgEYAAIBRAACQKVgAkDpYAJBGWACQUVyCVYA6AIErkDBMAJBRWIErgEwAVYBRAACQNVgAkEVUAJBRXIJVgFEAgSuARQBVkDpUAJBIVIErgDUAhlWAOgAAgEgAgSuQNVQAkEZUAJBRYIIAgCkAVYBRAIUrgDUAAIBGAIQAkClUAJA1UACQQVAAkEhYhgCASACCAIA1AIIAgEEAAJA1XACQT2iFK4BPAIJVgDUAAJA8WACQSFgAkEhYAJBNWIYAgEgAAIBNAIIAgEgAiACAPAAAkDxUAJBFWACQTVCCAIA8AIIAgE0AggCQPFQAkEFYAJBFWACQT1iCAIBFAACAPABVkDwMgSuAQQAAgEUAAIBPAFWAPABWgCkAVYAwAIIAkD4MggCAPgCCAJA8VACQQFgAkEhYAJBMWACQQ2iCVYA8AIMrgEMAVZA3RACQPFQAkEVYAJBLWACQS1iEAIBAAACASAAAgEwAAIA3AIMrkEhUVYA8AACARQAAgEsAhSuQKVQAkDBYAJA5UIIAgEgAhgCAKQCEAIA5AIIAkDxUiACAPACCAIAwAACQSFgAkEhYglWQRViCVpBNUFWQQ2SCAIBIAACASAAAkFBUAJBQVIIAgEMAhgCAUACCVYBFAACAUAABgE0AgSqQPFQAkEFgAJBKWIIAgDwAggCASgAAkDxYAJBMXIErgEEAVYA8AFWATAAAkEFcglaQPFhVgE0AAIA5AACQJkgAkEVcAJBNUACQOUiEVYBBAIJWkD5IVYBFAACQQViCAIA+AACQMlAAkEpYhSuAPAAAgEoAAJA6XACQPlgAkEFUAJBDXFWAQQCCVYAmAIErgDIAgyuAOgAAgD4AAIBBAACAQwBVkDxYAJBAWACQQ2iCAJBUXIgAgFQAhgCAPAAAgEAAkACAQwCUVZBIYIErkEFIiACAQQCOVYBLAACQRVyFK5BRVACQTVhVgEUAgSuQQ1iGAIBNAJBVgEgAgSuAQwAAkE1giACATQCbK5A8WACQNVhVkEhMggCANQCFK5BKYFWASACCAJA1WACQTViBK4A8AFWQMGyDK4BKAACQPEgAkEVQglWARQCCAIAwAACQQViCAJBURIErkDlQigCAPABVkEVUggCAQQCKAIBFAIQAkEFYgSuAVACEAIA5AACAQQCCVZBDXIIAgE0AiACQTWiCAIBDAIZVkENYhSuQJFiCAIBYAACQWFSGAIAkAIpVkEVQggCAQwCBK5BAWIQAgFEAVYBFAIErkDdQAJBKXIIAgEAAAIA3AIIAkCtYAJA3WACQOliEAIBKAACAOgCBK4A3AIJVkDdYAJBDWACQRlyCAIBNAACANwCCAIArAIErgEMAAIBGAJAAkFFkglWAQQAAkEFYgSuAUQBVkDxYhgCAPAAAkE9QAJBFSIUrgEUAglWATwAAkEFYAJBFWIgAgDUAAIBBAACARQAAkEVcggCQTUyEAIBNAIIAgEUAggCQNVgAkEFMAJBNVIJVgDUAgSuAQQAAgE0AigCQTFyIAIBMAACQSGAAkDVEAJBmWIIAkDlYVYBIAACQQVgAkE9ghFaAOQCBKoBPAIIAgDUAAIBBAACQTVyDK5BGfACQUmAAkFJgAJBPVIIAgFIAggCARgAAkEVkVYBNAACQPFiDK4BSAACATwAAgEUAggCQTFyEAJBIaFWAPACBK5A8WACQQFiCAIBMAIJVkDdQgSuASAAAkD5YAJArXIIAgDwAggCANwCCAIA+AIIAkElchSuASQCCVZBIXIIAkDcIjgCAKwCCAIBIAACQPESEVZA1WIErgDcAggCAPABVgDUAgyuAQAAAkDlYhACAOQCCAJAmVACQQVyEAJBFXIErkE1EVYAmAACQOUyGAIBBAIErkEpgVYBFAIQAgDkAVoBNAIUqkFFsjSuAUQCIAJBFWIJVgEUAgSuASgCEAJBCVACQTVSCVZBBWIoAgEEAVoBNAFWAQgCCVYBmAIgA/y8A');\n",
       "                        });\n",
       "                </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74aefa8c93dc44f1829a73149fa979d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=771.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    transformer.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in tqdm(train_loader):\n",
    "        # Move to GPU if available.\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Reset gradients.\n",
    "        transformer.zero_grad()\n",
    "        \n",
    "        # Prepare inputs and targets.\n",
    "        x = batch[:, :-1]\n",
    "        y = batch[:, 1:]\n",
    "\n",
    "        # Forward prop.\n",
    "        y_hat = transformer(x)\n",
    "\n",
    "        loss = loss_function(y_hat, y)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Back prop.\n",
    "        loss.backward()\n",
    "\n",
    "        # Step scheduler (and optimizer).\n",
    "        scheduler.step()\n",
    "        \n",
    "    if epoch % LOG_EVERY_N == 0:\n",
    "        avg_loss = total_loss / len(train_loader)  # per token loss\n",
    "        writer.add_scalar(\"train_loss\", avg_loss, global_step=epoch)\n",
    "    \n",
    "    if epoch % VAL_EVERY_N == 0:\n",
    "        ppl = validate_transformer(transformer, val_loader)\n",
    "        writer.add_scalar(\"val_ppl\", ppl, global_step=epoch)\n",
    "        \n",
    "        primer = torch.LongTensor([[BOS_TOKEN]]).to(device)\n",
    "        seq = transformer.generate(primer, length=1024)\n",
    "        display_audio(seq.tolist())\n",
    "\n",
    "    if epoch % SAVE_EVERY_N == 0:\n",
    "        state_checkpoint = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": transformer.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        }\n",
    "        torch.save(state_checkpoint, f\"models/checkpoints/ckpt_{epoch}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_checkpoint = {\n",
    "    \"epoch\": epoch,\n",
    "    \"model_state_dict\": transformer.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "}\n",
    "torch.save(state_checkpoint, f\"models/Transformer(256,390,4),lr=0.001,bsz=8,nepochs=1000,sl=1024,dropout=0.1,t=2020-11-14_12:52.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm = MusicRNN(\n",
    "    embedding_dim=64,\n",
    "    hidden_dim=256,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    rnn_type=\"lstm\",\n",
    "    num_layers=2,\n",
    "    dropout=0.5,\n",
    ").to(device)\n",
    "\n",
    "sd = torch.load(\"models/lstm(64,256),lr=0.001,bsz=256,nepochs=250,sl=1024,dropout=0.5,t=2020-11-13_14:30.pt\")[\"model_state_dict\"]\n",
    "lstm.load_state_dict(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <div id='midiPlayerDiv399'></div>\n",
       "                <link rel=\"stylesheet\" href=\"//cuthbertLab.github.io/music21j/css/m21.css\"\n",
       "                    type=\"text/css\" />\n",
       "                <script>\n",
       "                require.config({\n",
       "                    paths: {'music21': '//cuthbertLab.github.io/music21j/src/music21'}\n",
       "                });\n",
       "                require(['music21'], function() {\n",
       "                               mp = new music21.miditools.MidiPlayer();\n",
       "                               mp.addPlayer('#midiPlayerDiv399');\n",
       "                               mp.base64Load('data:audio/midi;base64,TVRoZAAAAAYAAQABBABNVHJrAAABCQD/AwAA4ABAAMAAAP9RAwehIAD/WAQEAhgIiACQK0wAkDdIAJA7TIgAgDsAAJAyRIIAgDcAhACQO0gAkD5MhFWAMgCBK5AyJACQN0iCAIA7AACAPgCEAIA3AFWQO0wAkD5MglWAOwCBK4A+AIMrkDdAglWANwCDK4AyAACQN1AAkDtUAJA+TACQR1SCAIBHAFWANwAAkEhMgSuAKwBVgEgAgSuQN1AAkElQggCAOwAAgD4AAIBJAIIAkEpUgyuQKFAAkDtMAJA+SFWASgCEAIA3AIIAkDc8gSuAOwAAgD4AVYA3AIQAkDdMAJA7UACQPkyCVYA3AIErgDsAAIA+AIcrgCgAiAD/LwA=');\n",
       "                        });\n",
       "                </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "FIRST_N = 125\n",
    "IDX = 10\n",
    "\n",
    "primer = test_songs[IDX][:FIRST_N].tolist()\n",
    "display_audio(primer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-75f4b2ae521e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# primer = torch.LongTensor([[BOS_TOKEN]]).to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprimer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_songs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIDX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFIRST_N\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdisplay_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transformer' is not defined"
     ]
    }
   ],
   "source": [
    "# primer = torch.LongTensor([[BOS_TOKEN]]).to(device)\n",
    "primer = test_songs[IDX][:FIRST_N].unsqueeze(0).to(device)\n",
    "seq = transformer.generate(primer, length=1024)\n",
    "display_audio(seq.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-57-07a2cea3d532>(44)generate_sequence()\n",
      "-> kpos_next_tokens = self._generate_next_token(\n",
      "(Pdb) n\n",
      "> <ipython-input-57-07a2cea3d532>(45)generate_sequence()\n",
      "-> candidates=torch.LongTensor(seq).to(device),\n",
      "(Pdb) n\n",
      "> <ipython-input-57-07a2cea3d532>(46)generate_sequence()\n",
      "-> reset_hidden=True,\n",
      "(Pdb) n\n",
      "> <ipython-input-57-07a2cea3d532>(47)generate_sequence()\n",
      "-> **kwargs\n",
      "(Pdb) n\n",
      "IndexError: invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number\n",
      "> <ipython-input-57-07a2cea3d532>(47)generate_sequence()\n",
      "-> **kwargs\n",
      "(Pdb) n\n",
      "--Return--\n",
      "> <ipython-input-57-07a2cea3d532>(47)generate_sequence()->None\n",
      "-> **kwargs\n",
      "(Pdb) n\n",
      "IndexError: invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number\n",
      "> <ipython-input-66-4577b91f5022>(2)<module>()\n",
      "-> continuation = lstm.generate_sequence(primer, temp=1.0, topk=128)\n",
      "(Pdb) q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-4577b91f5022>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprimer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_songs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIDX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFIRST_N\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcontinuation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdisplay_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontinuation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exception'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c_call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mdispatch_exception\u001b[0;34m(self, frame, arg)\u001b[0m\n\u001b[1;32m    172\u001b[0m                     and arg[0] is StopIteration and arg[2] is None):\n\u001b[1;32m    173\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0;31m# Stop at the StopIteration or GeneratorExit exception when the user\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;31m# has set stopframe in a generator by issuing a return command, or a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "primer = test_songs[IDX][:FIRST_N].tolist()\n",
    "continuation = lstm.generate_sequence(primer, temp=1.0, topk=128)\n",
    "display_audio(continuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
